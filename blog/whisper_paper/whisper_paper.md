# Whisper summary
In this article, we will summarize the new end-to-end speech recognition model developed by OpenAI: [Whisper](https://cdn.openai.com/papers/whisper.pdf).

## Introduction
Recent progress in Automatic Speech Recognition (ASR) have been primarily based on the approach dominating the latest years in Natural Language Processing (NLP) and Computer Vision (CV): transfer learning. This consits of pretrainig large models in a self-supervised fashion on massive datasets to perform one or several basic tasks. Then, one or more models are fine-tuned on specific tasks and datasets in a full supervised fashion.

According to the authours, this approach - although it led to models achieving state-of-the-art results on most benchmarks, often surpassing human baselines - has limitations. First, as said, models need to be fine-tuned on specific tasks, consuming resources and generating countless versions to manage. But most important, models fine-tuned on specific datasets have shown poor generalization even with minor changes in the target distribution.

In this paper, it is shown that ASR models pretrained with full or weak supervision on end tasks exhibit higher robustness and generalize better. As it is well known by every Machine Learning practicioner, labelling large datasets is expensive and tedious and therefore the trade-off between quality and quantity plays a crucial role. ASR makes no expection.

Whisper training is weakly supervised, meaning that - in most of the training datasets - the only information available is the input audio and the corresponding transcript.

![audio to transcript](blog/whisper_paper/images/audio-transcript.png)

The largest model (1.55B parameters trained on 680.000 hours) achieves great zero-shot results on gold standard benchmarks. We will see more about the results later on, but first let's have a look at the training data.

## Datasets
Similar to most of the recent text datasets, the one used to train Whisper is web-crawled. This results in a very diverse dataset covering a broad distribution of audio from many different environments, recording setups, speakers, and languages. However, many transcripts on the internet have been generated by other ASR models, which are harming the model's quality and have to be filtered out. The authours use a set of euristics to detect and clean the dataset from automatically generated transcripts.

Long audio files are splitted into 30 seconds chunks and paired with the subset of the transcript occurring within that time window. Authors furthermore use fuzzy de-duping of transcript texts to reduce the amount of duplication. Fuzzy-what? Maybe I should write something about this.

Most interestingly, transcripts are not preprocessed. The dataset size is such that this does not harm the model's performance: in fact, it removes the needs for an inverse text normalization postprocessing step (e.g.: `"my name is emilio and i love asr"` -> `"My name is Emilio, and I love ASR."`).
In simpler words, it means that the models predicts also punctuation, digits, and capitalazion.

Two separate language identification models are trained on specific datasets - one for audio, and one for text - and are used to ensure that corresponding audio-transcript pairs are in the same language. There is though an exception...

As said, the model is trained to map an input audio to an output transcript. However, considering the variety in the dataset, the tasks the model learns to perform are actually more, and include:
* transcribe in the same language (`transcribe`)
* transcribe in English (`translate`)
* predict silence (yeah, we don't always know a-priori that someone is actually speaking...)

The dataset contains 97 languages and is composed of:
* 65% (438.218h) English speech recognition (`English` audio -> `English` transcript)
* 18% (125.739h) Translation speech recognition (`X` audio -> `English` transcript)
* 17% (117.113h) X speech recognition (`X` audio -> `X` transcript)
where X is any of the other 96 languages. Please take a look at the paper's Appendix E for detailed training dataset statistic.

Now you should know which is above-mentioned exception: human-produced English transcripts are always preserved, so that the model learns to transcribe/translate (transcrate, translibe, I can't find a good word for that...) from any language to English.

Nice, now we have an idea about the fuel. But what about the engine? Well, let's find out.

## Model
As the research goal was to focus more on the impact of the training process, the authors chose a standard [encoder-decoder transformer model architecture](https://arxiv.org/pdf/1706.03762.pdf) which has shown to scale reliably. We will dive deep into its structure in one of the future posts, where we will investigate the impact of fine-tuning and optimisation techniques. For the scope of this article, let's just cover the main concepts.

### Transformer models
Transformer models are based on the attention mechanism, thanks to which they learn how different input features relate with each other to better represent them in the latent space. As in other deep-learning architectures, the input is first transformed into a suitable form - for example strings are mapped to a numerical representation - and then to a dense array, the so-called embedding. The embedded input is then propagated through an encoder, a decoder, or both, before being passed to the model's head. Encoders and decoders of a standard transformer are built as stacks of encoder and decoder layers respectively: all layers have the exact same architecture, but do not share weights.

Encoder-only models propagate the whole input through the encoder at once, and its output is then passed through the model's head, which is designed and trained to solve one or more specific tasks.

![encoder architecture](blog/whisper_paper/images/encoder.png)

Decoder-only models are similar, but the output is generated one step at the time, thus the model cannot look at future values. Each newly generated output is concatenated to its input, and used as input in the next iteration.

![decoder architecture](blog/whisper_paper/images/decoder.png)

Encoder-decoder models like the origianl Transformer, Whisper, and many others, are often used to solve sequence-to-sequence tasks (speech-to-text, translation, summarization, ...) and combine both blocks. The whole input is first passed through the encoder, and the encoder output is passed to each of the decoder's layers. The decoder receives also its own input sequence, which is used to predict the next token, together with the information received by the encoder. The predicted token gets concatenated to the previous ones, and the new sequence is passed through the decoder again. This process continues until an "end of sequence" token is predicted, or other stopping point is reached.

![encoder-decoder architecture](blog/whisper_paper/images/encoder-decoder.png)

### Audio preprocessing
In the specific architecture, the input audio wave is initially transformed into a log-mel spectrogram. In a few words, the audio - which is originally represented as a sequence of values over time - is translated into its frequency representation, which means that...

![log-mel spectrogram](blog/whisper_paper/images/log_mel.png)

### Text preprocessing
As mentioned in the model chapeter's first paragraph, machine learning models are not suitable for handling strings, neither as input nor as output. The job of splitting strings into tokens (single characters, punctuations, numbers, subwords, words) and mapping tokens to integers, and viceversa, is handled by the tokenizer.

```python
>>> original_input = "This is a tokenization example"
>>> token_ids = processor.tokenizer.encode(original_input)
>>> tokens = processor.tokenizer.batch_decode(token_ids)
>>> reconstructed_input = processor.tokenizer.decode(token_ids)

>>> original_input
'This is a tokenization example'

>>> tokens
['This', ' is', ' a', ' ', 't', 'oken', 'ization', ' example']

>>> token_ids
[5723, 307, 257, 220, 83, 8406, 2144, 1365]

>>> reconstructed_input
'This is a tokenization example'
```

A token is simply the smallest unit the model is going to handle. A key parameter is the vocabulary size, which is the number of different tokens the tokenizer is aware of. Whisper uses the BPE tokenizer which was trained for GPT2 when handling English text, and other tokenizers for other languages. ADD MORE. The way tokenizers learn to split sentences into words or subwords goes beyond the scope of the article, but I hope you will soon understand how it fits into the picture.

Simply representing a text as a sequence of integers is not ideal. Think about this minimal example, and remember that models "simply" process numbers: 
```python
>>> token_ids = tokenize(["Whisper", "is", "a", "great", "model"])
>>> token_ids
[1,2,3,4,5]
```
What does this mean to the model? Is the word `"model"` five times more important than the word `"Whisper"`? One solution would be to represent the inputs with one-hot vectors, which are vectors of all zeros, and a one in the position corresponding to the integer to be represented. 

```python
>>> one_hot_encodings = one_hot_encoding(token_ids)
>>> one_hot_encodings
[[1,0,0,0,0],
 [0,1,0,0,0],
 [0,0,1,0,0],
 [0,0,0,1,0],
 [0,0,0,0,1]]
```

In fact, a one-hot encoding is usually input format with which tokenized texts are passed to the models. As you can imagine, such representation with a vocabulary of tens of thousands of words leads to a very sparse representation, which harms the model performance. To cope with this issue, the matrix is fed through an embedding layer, which transforms it to a dense representation.
Here, the key parameter is the embedding dimensionality.

```python
>>> embedding_size = 10
>>> embeddings = embed(one_hot_encodings)
>>> embeddings
[[0.67539001, -0.02781102, 0.7405842, 0.92543957, -0.82743469,
 -0.8399026, -0.90059809, -0.1726493, -0.44334005, -0.69554916],
 [-0.95575285, 0.79144297, -0.06387612, 0.88995377, 0.11384941,
 0.87697974, -0.62919236, 0.64791116, -0.90305178, -0.59374918],
 [ 0.17804501, -0.11817887, -0.74232328, 0.40329305, -0.10807508,
 -0.05667099, -0.64122679, -0.96651145, 0.99033302, 0.81605493],
 [ 0.47344379, -0.59197299, -0.88876534, 0.12615804, 0.91232643,
 0.89029839, 0.40303782, -0.19380694, 0.29575598, -0.97647317],
 [ 0.95680021, 0.23221837, 0.56459264, -0.85751143, -0.65173394,
 -0.99427018, 0.43448618, 0.81102759, 0.35889378, -0.06338368]]
```

So, if we tokenize and embed `"This is a tokenization example"` using Whisper large model's tokenizer and embedding layer, we get the following representations:
```python
>>> t = torch.tensor(token_ids)
>>> t
tensor([5723,  307,  257,  220,   83, 8406, 2144, 1365])
>>> t.shape
torch.Size([8])

>>> embeddings = model.model.decoder.embed_tokens(t)
>>> embeddings
tensor([[-0.0014, -0.0077,  0.0017,  ..., -0.0049,  0.0080,  0.0095],
        [ 0.0061,  0.0101, -0.0050,  ...,  0.0026, -0.0058,  0.0113],
        [-0.0031,  0.0004,  0.0324,  ...,  0.0125, -0.0039,  0.0029],
        ...,
        [-0.0028, -0.0133, -0.0061,  ..., -0.0118,  0.0086,  0.0071],
        [-0.0139, -0.0101, -0.0157,  ..., -0.0128,  0.0046,  0.0005],
        [-0.0026, -0.0127, -0.0026,  ..., -0.0009,  0.0128,  0.0120]],
       grad_fn=<EmbeddingBackward0>)
>>> embeddings.shape
torch.Size([8, 1280])
```

This is how text inputs are transformed before being fed to the first decoder layer.

### Encoder
Whisper's encoder thus receives the preprocessed audio input, and transforms it by a forward pass through its layers.

### Decoder
Whisper's decoder instead receives, at each iteration, the preprocessed tokens which have been predicted until the previous iteration. Furthermore, each layer receives the encoder's output, as shown in the last image of the [Transformer models](#transformer-models) paragraph of this chapter.

## Tasks
In Whisper, each inference is initialized with a sequence of `start` tokens, of which the first is `"<|startoftranscript|>"`.

Next, the language is provided. In practice, though, Whisper can also be used to perform langauge identification. Since the model was trained to predict sequences which were always starting with ["<|startoftranscript|>", <|languagecode|>, ...], Whisper will predict, as second token, the language it thinks is spoken in the audio segment.

After, comes the main task. Since the authors refer to `transcribe` as the task to transcribe the content of an audio file in the same language, and to `translate` as the task to transcribe the audio file in English, when English was not the language spoken, the third token should either be `"<|transcribe|>"` or `"<|translate|>"`.

Last, the model was trained to predict tokens timestamps when available in the training data, and a `"<|notimestamp|>"` token was added after the main task when no such information was available. Timestamps are predicted relative to the current audio segment and quantised to 20 milliseconds, and added to the vocabulary. The model then predicts: [..., `token_start_time`, `token`, `token_end_time`]. If the token audio is not entirely included in the 30 seconds chunk, then no `token_end_time` will be predicted. This indicates that the next segment should be shifted back to include the beginning of the last token of the current chunk, for it allows accurate long-audio transcriptions.

Each transcript was closed with a `"<|endoftranscript|>"` token. Whisper predicting it indicates that the transcription is completed, and no further tokens should be predicted. It is the stop signal.

## Evaluation

## So, what's next?